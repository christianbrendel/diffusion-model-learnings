{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Denoising Process</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_tensors as lt\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ldm.modules.encoders.modules import FrozenOpenCLIPEmbedder\n",
    "from ldm.modules.diffusionmodules.openaimodel import UNetModel\n",
    "from ldm.models.autoencoder import AutoencoderKL\n",
    "\n",
    "from helper import ForwardDiffusionProcessor, X2imgs, imgs2X\n",
    "\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod the weights\n",
    "\n",
    "ckpt = \"../v2-1_512-ema-pruned.ckpt\"\n",
    "state_dict = torch.load(ckpt, map_location=\"cpu\")[\"state_dict\"]\n",
    "\n",
    "state_dict_vae = {}\n",
    "state_dict_unet = {}\n",
    "state_dict_clip_embedder = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"first_stage_model.\"):\n",
    "        state_dict_vae[k.replace(\"first_stage_model.\", \"\")] = v\n",
    "    elif k.startswith(\"model.diffusion_model.\"):\n",
    "        state_dict_unet[k.replace(\"model.diffusion_model.\", \"\")] = v\n",
    "    elif k.startswith(\"cond_stage_model.\"):\n",
    "        state_dict_clip_embedder[k.replace(\"cond_stage_model.\", \"\")] = v\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the parameters\n",
    "config_path = \"../configs/stable-diffusion/v2-inference.yaml\"\n",
    "cfg = OmegaConf.load(config_path)\n",
    "\n",
    "# load the variational autoencoder\n",
    "vae = AutoencoderKL(**cfg.model.params.first_stage_config.params)\n",
    "vae.load_state_dict(state_dict_vae, strict=True)\n",
    "vae.to(device)\n",
    "vae.eval()\n",
    "\n",
    "# load the ddpm (denoising diffusion probabilistic model)\n",
    "unet = UNetModel(**cfg.model.params.unet_config.params)\n",
    "unet.load_state_dict(state_dict_unet, strict=True)\n",
    "unet.to(device)\n",
    "unet.eval()\n",
    "\n",
    "# load the clip embedder\n",
    "clip_embedder = FrozenOpenCLIPEmbedder(**cfg.model.params.cond_stage_config.params)\n",
    "clip_embedder.load_state_dict(state_dict_clip_embedder, strict=True)\n",
    "clip_embedder.to(device)\n",
    "clip_embedder.eval()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a ForwardDiffusionProcessor, which is needed to get the corresponding\n",
    "# alhpa values for out denoising process.\n",
    "fdp = ForwardDiffusionProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed to get reproducible results.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Prompts from lexica.art\n",
    "# prompt = \"portrait of a dark god, copper wires, visible scars and nerves, intricate, headshot, highly detailed, digital painting, artstation, concept art, sharp focus, cinematic lighting, illustration, art by artgerm and greg rutkowski, alphonse mocha, cgsociety, Olivia\"\n",
    "# prompt = \"portrait of a female pinup model in love in tropical Thailand forest lake wearing a golden Swimsuit, year 1977 with hallucinatory experience, lush magic plants, armani style : : by martine johanna and simon stalenhag and chie yoshii and casey weldon and wlop : : ornate, dynamic, particulate, rich colors, intricate, elegant, highly detailed, the vogue, harper's bazaar art, fashion magazine, smooth, sharp focus, 8 k, octane render, style of sports illustrated\"\n",
    "# prompt = \"apex legends assassin in tactical jumpsuit. concept art by james gurney and mÅ“bius. cinematic, dramatic lighting\"\n",
    "\n",
    "# Parameters\n",
    "prompt = \"a beautiful border collie, marco lens, dslr, photorealistic\"\n",
    "n_steps = 100\n",
    "guidance_scale = 15\n",
    "batch_size = 3\n",
    "\n",
    "# Generate a random noise to start the denoising process with.\n",
    "embedding_shape = (batch_size, 4, 64, 64)\n",
    "Z = torch.randn(embedding_shape).to(device)\n",
    "\n",
    "# Generate the prompt embedding. Because we are using classifier-free guidance,\n",
    "# we need to generate two embedding, i.e. the embedding of our prompt and the\n",
    "# embedding of the unconditioned prompt.\n",
    "with torch.no_grad():\n",
    "    C = clip_embedder(batch_size * [prompt])\n",
    "    C_unconditioned = clip_embedder(batch_size * [\"\"])\n",
    "\n",
    "# Generate the timesteps.\n",
    "timesteps = fdp.get_timesteps(n_steps)\n",
    "\n",
    "# Denoise the image.\n",
    "Z_evo = []\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():    \n",
    "\n",
    "    # for ts in timesteps[::-1]:\n",
    "    for i in reversed(range(n_steps)):\n",
    "\n",
    "        ts = timesteps[i]\n",
    "        ts_prev = timesteps[i - 1] if i > 0 else ts        \n",
    "        print(f\"{i:>4} | ts={ts:<3} -> ts_prev={ts_prev:<3}\", end=\"\\r\")\n",
    "\n",
    "        # Encode the timestep.\n",
    "        TS = torch.full((batch_size,), ts, device=device, dtype=torch.long)\n",
    "\n",
    "        # Estimate the noise on the image. We do this twice, once conditioned on\n",
    "        # the prompt and once unconditioned.\n",
    "        EPS, EPS_unconditioned = unet(\n",
    "            torch.cat([Z] * 2), \n",
    "            torch.cat([TS] * 2), \n",
    "            torch.cat([C, C_unconditioned])\n",
    "            ).chunk(2)\n",
    "\n",
    "        # Combine the predicted conditioned and unconditioned noise to guide the\n",
    "        # denoising process.\n",
    "        EPS = EPS_unconditioned + guidance_scale * (EPS - EPS_unconditioned)\n",
    "\n",
    "        # Calculate the denoised image. The corresponding formula can be found\n",
    "        # in the following paper or in the blog post from Lilian Weng:\n",
    "        # - https://arxiv.org/abs/2010.02502\n",
    "        # - https://lilianweng.github.io/posts/2021-07-11-diffusion-models/    \n",
    "        alpha_bar = torch.full((batch_size, 1, 1, 1), fdp.get_alpha_bar(ts), device=device)\n",
    "        alpha_bar_prev = torch.full((batch_size, 1, 1, 1), fdp.get_alpha_bar(ts_prev), device=device)\n",
    "        Z = alpha_bar_prev.sqrt() * (Z - (1 - alpha_bar).sqrt()*EPS) / alpha_bar.sqrt() + (1-alpha_bar_prev).sqrt() * EPS\n",
    "        Z_evo.append(Z)\n",
    "\n",
    "    # Decode the latent vector to get the final image.      \n",
    "    Z_ = 1.0 / cfg.model.params.scale_factor * Z\n",
    "    X = vae.decode(Z_)\n",
    "    imgs = X2imgs(X)    \n",
    "    \n",
    "plt.figure(figsize=(batch_size*4, 4))\n",
    "plt.imshow(np.hstack(imgs))\n",
    "plt.axis(\"off\")\n",
    "plt.title(prompt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the gif\n",
    "\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "\n",
    "imgs_evo = []\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():    \n",
    "    for Z in tqdm(Z_evo):\n",
    "        Z_ = 1.0 / cfg.model.params.scale_factor * Z\n",
    "        X = vae.decode(Z_)\n",
    "        imgs_evo.append(X2imgs(X))\n",
    "\n",
    "imgs_evo = np.array([np.hstack(imgs) for imgs in imgs_evo])\n",
    "imageio.mimsave(\"imgs_evo.gif\", imgs_evo, fps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4895cdc8b443a3a23981640910a32e7a59b10b569d9d0b54f5319973a233b4b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
